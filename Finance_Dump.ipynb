{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import csv\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import pyplot as plt\n",
    "%pylab inline\n",
    "import string\n",
    "import re\n",
    "pd.options.display.float_format = '${:10,.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_xl_q3 = pd.read_excel('C:/Jeyaraj/Booking_Dump/Commercial_Booking_FY16Q3 (downloaded on 2016-07-01).xlsx')\n",
    "#df_xl_q4 = pd.read_excel('C:/Jeyaraj/Booking_Dump/Commercial_Booking_FY16Q4M2 (downloaded on 2016-07-01).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = 'SELECT * FROM xldump_to_sqldump_map2'\n",
    "df_sql_cols = pd.read_sql(sql, conn)\n",
    "col_dict = {}\n",
    "for index, rows in df_sql_cols.iterrows():\n",
    "    col_dict[rows[1].lower()] = rows[2]\n",
    "#print col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_xl_q3.rename(columns=string.lower, inplace=True)\n",
    "df_xl_q3.rename(columns=col_dict, inplace=True)\n",
    "df_xl_q3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql2 = 'SELECT MAX(ID) AS max_id FROM ent_dump_from_finance2';\n",
    "df = pd.read_sql_query(sql2, conn)\n",
    "max_id = df.loc[0]\n",
    "max_id = max_id + 1\n",
    "total_rows = len(df_xl_q3.index)\n",
    "end_id = max_id + total_rows\n",
    "index_id = Series([x for x in range(max_id, end_id)])\n",
    "print max_id\n",
    "print len(index_id)\n",
    "df_xl_q3['id'] = index_id\n",
    "#index_id\n",
    "#print \"Max ID in ent_dump_finance2 is %d\" % num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_xl_q3.to_sql(name='ent_dump_from_finance2', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate MySQL connection\n",
    "engine_url = 'mysql://root:Jey03$78@localhost/mysourcedata?charset=utf8'\n",
    "engine = create_engine(engine_url, pool_size=100, pool_recycle=3600, echo_pool=True)\n",
    "conn = engine.connect()\n",
    "\n",
    "sql = 'SELECT * FROM ent_dump_from_finance WHERE LEFT(Fiscal_Quarter_ID, 4)=\"2016\"'\n",
    "#sql = 'SELECT * FROM ent_dump_from_finance WHERE Fiscal_Quarter_ID=\"2016Q3\"'\n",
    "df_ent = pd.read_sql_query(sql, conn)\n",
    "df_ent.rename(columns=string.lower, inplace=True)\n",
    "pivoted = df_ent.pivot_table(index=['fiscal_quarter_id', 'fiscal_period_id'], columns=['sales_level_3','services_indicator'], values='booking_net', aggfunc=sum, margins=True, fill_value=0)\n",
    "pivoted\n",
    "#if df_ent.empty:\n",
    "#    print 'DF is empty'\n",
    "#else:\n",
    "#    print 'DF has got items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make a ent_dump_from_finance2 (Temporary Table)\n",
    "import MySQLdb\n",
    "db = MySQLdb.connect('localhost', 'root', 'Jey03$78', 'mysourcedata')\n",
    "cursor = db.cursor()\n",
    "def createLikeTable(new_tbl_name, like_tbl_name, cursor):\n",
    "    sql = 'CREATE TABLE IF NOT EXISTS '+new_tbl_name+' LIKE '+like_tbl_name\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "def cloneENT_DFF(year, cursor):\n",
    "    sql = 'INSERT INTO ent_dump_from_finance2 SELECT * FROM ent_dump_from_finance WHERE LEFT(Fiscal_Quarter_ID,4)=%d' % (year)\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        db.commit()\n",
    "        print 'Committed!'\n",
    "    except MySQLdb.Error, e:\n",
    "        db.rollback()\n",
    "        print \"MySQL Error [%d]: %s\" % (e.args[0], e.args[1])\n",
    "    sql2 = 'SELECT COUNT(*) FROM ent_dump_from_finance2'\n",
    "    cursor.execute(sql2)\n",
    "    result = cursor.fetchone()\n",
    "    print 'Following Record(s) have been appended for the year %d!' % (year)\n",
    "    print result\n",
    "\n",
    "def segmentizeCloning():\n",
    "    db = MySQLdb.connect('localhost', 'root', 'Jey03$78', 'mysourcedata')\n",
    "    cursor = db.cursor()\n",
    "    for year in [2016]:\n",
    "        cloneENT_DFF(year, cursor)\n",
    "    db.close()\n",
    "\n",
    "#createLikeTable('ent_dump_from_finance2', 'ent_dump_from_finance', cursor)\n",
    "segmentizeCloning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def readExcelGetDataFrame(absolute_path, file_name, sheet_name, field_to_upper='customer_name'):\n",
    "    expanded_file_name = absolute_path + file_name + '.xlsx' \n",
    "    print 'Reading %s...' % expanded_file_name\n",
    "    xl = pd.ExcelFile(expanded_file_name)\n",
    "    df = xl.parse(sheet_name)\n",
    "    df.rename(columns=string.lower, inplace=True)\n",
    "    # df[field_to_upper] = df[field_to_upper].str.upper()\n",
    "    return df\n",
    "\n",
    "\n",
    "is_comm = True\n",
    "year = 2016\n",
    "absolute_path = 'C:/Jeyaraj/Analysis/PBG_Dashboards/pandas/'\n",
    "list_df = []\n",
    "file_name_txt = 'com_booking_mapped' if is_comm else 'ent_booking_mapped'\n",
    "for quarter in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "    print 'Reading %s data...' % (str(year)+quarter)\n",
    "    try:\n",
    "        df = readExcelGetDataFrame(absolute_path, file_name_txt+str(year)+quarter, 'raw_data')\n",
    "        print 'Mapped Data of %s contains %d row(s).' % ((str(year)+quarter), len(df.index))\n",
    "        list_df.append(df)\n",
    "        print 'Completed reading %s data!' % (str(year)+quarter)\n",
    "    except:\n",
    "        print 'Something went wrong!'\n",
    "        traceback.print_exc()\n",
    "print 'Joining Quarter DFs into One DF...!'\n",
    "df_c = pd.concat(list_df)\n",
    "print 'Mapped Data %d row(s).' % (len(df_c.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readExcelGetDataFrame(absolute_path, file_name, sheet_name, field_to_upper='customer_name'):\n",
    "    expanded_file_name = absolute_path + file_name + '.xlsx' \n",
    "    print 'Reading %s...' % expanded_file_name\n",
    "    xl = pd.ExcelFile(expanded_file_name)\n",
    "    df = xl.parse(sheet_name)\n",
    "    df.rename(columns=string.lower, inplace=True)\n",
    "    # df[field_to_upper] = df[field_to_upper].str.upper()\n",
    "    return df\n",
    "\n",
    "#df_c.to_csv('../com_booking_mapped_2016.csv', sep=',', encoding='utf8')\n",
    "#absolute_path = 'C:/Jeyaraj/Analysis/PBG_Dashboards/pandas/'\n",
    "#df_sos = readExcelGetDataFrame(absolute_path, 'sos_mapping_list', 'sos','mapped_name')\n",
    "#fields = ['gaining_sales_level_2', 'gaining_sales_level_3', 'gaining_sales_level_4', 'gaining_sales_level_5', \\\n",
    "#        'gaining_sales_level_6', 'mapable', 'mapped_name', 'remarks', 'where_to', 'gaining_sub_scms']\n",
    "\n",
    "#df_sos.head()\n",
    "#df_c.ix[df_c.sales_order_number_detail ==  rows['sales_order_number_detail'], field] = rows[field]\n",
    "#df_c.ix[df_c.sales_order_number_detail ==  102133848]\n",
    "for index, rows in df_sos.iterrows():\n",
    "    #if rows['sales_order_number_detail'] == 102133848:\n",
    "    print rows['sales_order_number_detail'], rows['remarks']\n",
    "    for field in fields:\n",
    "        #print field\n",
    "        df_c.ix[df_c.sales_order_number_detail ==  rows['sales_order_number_detail'], field] = rows[field]\n",
    "print df_c.ix[df_c.sales_order_number_detail ==  102134337]\n",
    "\n",
    "#break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print df_c['sales_order_number_detail'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_sonos(cell_value):\n",
    "    pattern = re.compile(r\"[\\(,\\s]*(\\d{5,})[\\),\\s]\")\n",
    "    match = pattern.findall(cell_value)\n",
    "    return match\n",
    "\n",
    "def get_sos_file_to_dict(file_names):\n",
    "    data_dict = {}\n",
    "    for filename in file_names:\n",
    "        df = pd.read_excel('../'+filename)\n",
    "        for index, row in df.iterrows():\n",
    "            temp_dict = {row[0] : row[2]}\n",
    "            data_dict.update(temp_dict)\n",
    "    return data_dict\n",
    "    \n",
    "def get_sos_file_to_dict_by_pattern(file_name):\n",
    "    data_dict = {}\n",
    "    df = pd.read_excel('../'+file_name)\n",
    "    df.rename(columns=string.lower, inplace=True)\n",
    "    for index, row in df.iterrows():\n",
    "        if row['remarks'] != '' and pd.notnull(row['remarks']):\n",
    "            cus_name = row[0]\n",
    "            so_nos = extract_sonos(row['remarks'])\n",
    "            temp_dict = {so_no: cus_name for so_no in so_nos}\n",
    "            data_dict.update(temp_dict)\n",
    "    return data_dict\n",
    "    \n",
    "def get_so_nos_from_file():\n",
    "    data_dict = {}\n",
    "    # Get SO_Nos from Vipul's update file\n",
    "    data_dict.update(get_sos_file_to_dict_by_pattern('vipul_feedback.xlsx'))\n",
    "    \n",
    "    # Get ADP/Fidelity/Amway so numbers into Dataframe\n",
    "    filenames = ['adp_sos.xlsx','fidelity_sos.xlsx','amway_sos.xlsx']\n",
    "    data_dict.update(get_sos_file_to_dict(filenames))\n",
    "\n",
    "    df_final = DataFrame.from_dict(data_dict, orient='index') # Dictionary to Dataframe will not have column names\n",
    "    df_final.columns = ['mapped_name'] # Assign column names\n",
    "    df_final['sales_order_number_detail'] = df_final.index # Make Index as column\n",
    "    # Add More columns to the existing df_final dataframe\n",
    "    empty_columns = ['mapped_name','mapable', 'gaining_sub_scms', 'remarks','where_to','gaining_sales_level_2','gaining_sales_level_3','gaining_sales_level_4','gaining_sales_level_5','gaining_sales_level_6']\n",
    "    df_empty = pd.DataFrame(columns=empty_columns)\n",
    "    \n",
    "    df_final = pd.concat([df_final, df_empty]) # Concatenate df_final and df_empty to make df_final with all reqrd. columns\n",
    "    srs_mapped_name = df_final['mapped_name'].unique() # Assign all unique customer_names from df_final into a list\n",
    "    \n",
    "    # Iterate over customers list, obtain other fields data from mapping list dataframe and assign them in df_final\n",
    "    rqd_fields = ['mapable', 'mapped_name', 'gaining_sub_scms', 'remarks','where_to','gaining_sales_level_2','gaining_sales_level_3','gaining_sales_level_4','gaining_sales_level_5','gaining_sales_level_6']\n",
    "    for name in srs_mapped_name:\n",
    "        df_temp = df_c[df_c.mapped_name.str.contains(name, case=False).fillna(False)]\n",
    "        if not df_temp.empty:\n",
    "            df_temp = df_temp[rqd_fields]\n",
    "            rqd_data = [df_temp.mapable.unique()[0],df_temp.mapped_name.unique()[0],df_temp.gaining_sub_scms.unique()[0],df_temp.remarks.unique()[0],df_temp.where_to.unique()[0],df_temp.gaining_sales_level_2.unique()[0],df_temp.gaining_sales_level_3.unique()[0],df_temp.gaining_sales_level_4.unique()[0],df_temp.gaining_sales_level_5.unique()[0],df_temp.gaining_sales_level_6.unique()[0]]\n",
    "            df_final.ix[df_final.mapped_name.str.contains(name, case=False).fillna(False),rqd_fields] = rqd_data\n",
    "    \n",
    "    # Resetting and droping Index being sales order number and keep normal index\n",
    "    df_final.reset_index(inplace=True) \n",
    "    df_final.drop(['index'], inplace=True, axis=1)\n",
    "    df_final['sales_order_number_detail'] = df_final['sales_order_number_detail'].astype(int)\n",
    "    writer = pd.ExcelWriter('../sos_mapping_list.xlsx', engine='xlsxwriter')\n",
    "    df_final.to_excel(writer, sheet_name='sos')\n",
    "    writer.save()\n",
    "    #return df_final\n",
    "\n",
    "get_so_nos_from_file()\n",
    "\n",
    "\n",
    "#extract_sonos(df_vipul.remarks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = ['mapped_name']\n",
    "cols = ['services_indicator']\n",
    "#pivoted = df_c.pivot_table(values='booking_net', index=rows, columns=cols, aggfunc=sum, fill_value=0, margins=True)\n",
    "#pivoted\n",
    "grouped = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_anil = pd.read_excel('../anil_restated.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_com = pd.read_excel('../com_booking_mapped_2016.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ent = pd.read_excel('../ent_booking_mapped_2016.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281324"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_com.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204123"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ent.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_level_6</th>\n",
       "      <th>new_sales_level_3</th>\n",
       "      <th>new_sales_level_4</th>\n",
       "      <th>new_sales_level_5</th>\n",
       "      <th>new_sales_level_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IN_COM_S_BLR_SL_TM</td>\n",
       "      <td>INDIA_COMM_1</td>\n",
       "      <td>INDIA_COMM_SL_MM</td>\n",
       "      <td>IN_S_BLR_SL</td>\n",
       "      <td>IN_COM_S_BLR_SL_TM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN_COM_S_AP_TS_SL_TM</td>\n",
       "      <td>INDIA_COMM_1</td>\n",
       "      <td>INDIA_COMM_SL_MM</td>\n",
       "      <td>IN_S_AP_TS_MM</td>\n",
       "      <td>IN_COM_S_AP_MM_TM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN_COM_S_TN_SL_TM</td>\n",
       "      <td>INDIA_COMM_1</td>\n",
       "      <td>INDIA_COMM_SL_MM</td>\n",
       "      <td>IN_S_TN_MM</td>\n",
       "      <td>IN_COM_S_TN_MM_TM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IN_COM_W_MUM_SL_TM</td>\n",
       "      <td>INDIA_COMM_1</td>\n",
       "      <td>INDIA_COMM_SL_MM</td>\n",
       "      <td>IN_W_MUM_SL</td>\n",
       "      <td>IN_COM_W_MUM_SL_TM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IN_COM_W_PRG_SL_TM</td>\n",
       "      <td>INDIA_COMM_1</td>\n",
       "      <td>INDIA_COMM_SL_MM</td>\n",
       "      <td>IN_W_PRG_SL</td>\n",
       "      <td>IN_COM_W_PRG_SL_TM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sales_level_6 new_sales_level_3 new_sales_level_4 new_sales_level_5  \\\n",
       "0    IN_COM_S_BLR_SL_TM      INDIA_COMM_1  INDIA_COMM_SL_MM       IN_S_BLR_SL   \n",
       "1  IN_COM_S_AP_TS_SL_TM      INDIA_COMM_1  INDIA_COMM_SL_MM     IN_S_AP_TS_MM   \n",
       "2     IN_COM_S_TN_SL_TM      INDIA_COMM_1  INDIA_COMM_SL_MM        IN_S_TN_MM   \n",
       "3    IN_COM_W_MUM_SL_TM      INDIA_COMM_1  INDIA_COMM_SL_MM       IN_W_MUM_SL   \n",
       "4    IN_COM_W_PRG_SL_TM      INDIA_COMM_1  INDIA_COMM_SL_MM       IN_W_PRG_SL   \n",
       "\n",
       "    new_sales_level_6  \n",
       "0  IN_COM_S_BLR_SL_TM  \n",
       "1   IN_COM_S_AP_MM_TM  \n",
       "2   IN_COM_S_TN_MM_TM  \n",
       "3  IN_COM_W_MUM_SL_TM  \n",
       "4  IN_COM_W_PRG_SL_TM  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xl = pd.ExcelFile('../node_mapping.xlsx')\n",
    "#df_new_nodes = xl.parse('Sheet2')\n",
    "#len(df_new_nodes.index)\n",
    "df_new_nodes.drop(['sales_level_3', 'sales_level_4', 'sales_level_5'], axis=1, inplace=True)\n",
    "df_new_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_com = df_com.merge(df_new_nodes, on='sales_level_6', how='left')\n",
    "#df_ent = df_ent.merge(df_new_nodes, on='sales_level_6', how='left')\n",
    "#len(df_ent.index)\n",
    "#df_ent.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_com_prod = df_com[df_com.services_indicator == 'N']\n",
    "df_ent_prod = df_ent[df_ent.services_indicator == 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453858083.56290454"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_a = df_anil.groupby('fiscal_quarter_id')['booking_net'].sum()\n",
    "grouped_a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230125257.48120773"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_c = df_com_prod.groupby('fiscal_quarter_id')['booking_net'].sum()\n",
    "grouped_c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223732826.07963723"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_e = df_ent_prod.groupby('fiscal_quarter_id')['booking_net'].sum()\n",
    "grouped_e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453858083.5608448"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_f = df_2016.groupby(['fiscal_quarter_id'])['booking_net'].sum()\n",
    "grouped_f.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "bookings_adjustments_code\n",
      "bookings_adjustments_description\n",
      "bookings_adjustments_type\n",
      "at_attach\n",
      "business_unit\n",
      "customer_name\n",
      "erp_deal_id\n",
      "sales_order_number_detail\n",
      "fiscal_period_id\n",
      "fiscal_quarter_id\n",
      "fiscal_week_id\n",
      "partner_name\n",
      "product_family\n",
      "product_id\n",
      "Sales Agent Name\n",
      "sales_level_2\n",
      "sales_level_3\n",
      "sales_level_4\n",
      "sales_level_5\n",
      "sales_level_6\n",
      "scms\n",
      "sub_scms\n",
      "tms_level_1_sales_allocated\n",
      "tms_level_2_sales_allocated\n",
      "tms_level_3_sales_allocated\n",
      "tms_level_4_sales_allocated\n",
      "technology_group\n",
      "partner_tier_code\n",
      "partner_certification\n",
      "partner_type\n",
      "bill_to_site_city\n",
      "ship_to_city\n",
      "cbn_flag\n",
      "booking_net\n",
      "internal_business_entity_name\n",
      "internal_sub_business_entity_name\n",
      "services_indicator\n",
      "SAV Name\n",
      "mapable\n",
      "mapped_name\n",
      "gaining_sub_scms\n",
      "remarks\n",
      "where_to\n",
      "gaining_sales_level_2\n",
      "gaining_sales_level_3\n",
      "gaining_sales_level_4\n",
      "gaining_sales_level_5\n",
      "gaining_sales_level_6\n",
      "booking_net_'000\n"
     ]
    }
   ],
   "source": [
    "for each in df_anil.columns:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_new_com_cols(df):\n",
    "    #df.gaining_sales_level_2.fillna(df.new_sales_level_2, inplace=True)\n",
    "    df.gaining_sales_level_3.fillna(df.sales_level_3, inplace=True)\n",
    "    df.gaining_sales_level_4.fillna(df.sales_level_4, inplace=True)\n",
    "    df.gaining_sales_level_5.fillna(df.sales_level_5, inplace=True)\n",
    "    df.gaining_sales_level_6.fillna(df.sales_level_6, inplace=True)\n",
    "\n",
    "    df.gaining_new_sales_level_3.fillna(df.new_sales_level_3, inplace=True)\n",
    "    df.gaining_new_sales_level_4.fillna(df.new_sales_level_4, inplace=True)\n",
    "    df.gaining_new_sales_level_5.fillna(df.new_sales_level_5, inplace=True)\n",
    "    df.gaining_new_sales_level_6.fillna(df.new_sales_level_6, inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_new_com_cols2(df):\n",
    "    #df.gaining_sales_level_2.fillna(df.sales_level_2, inplace=True)\n",
    "    df.gaining_sales_level_3.fillna(df.sales_level_3, inplace=True)\n",
    "    df.gaining_sales_level_4.fillna(df.sales_level_4, inplace=True)\n",
    "    df.gaining_sales_level_5.fillna(df.sales_level_5, inplace=True)\n",
    "    df.gaining_sales_level_6.fillna(df.sales_level_6, inplace=True)\n",
    "    return df\n",
    "\n",
    "#pvt_com = df_com.pivot_table(values='booking_net', index=['services_indicator'], aggfunc=sum, margins=True)\n",
    "#pvt_com\n",
    "\n",
    "#pvt_ent = df_ent.pivot_table(values='booking_net', index=['services_indicator'], aggfunc=sum, margins=True)\n",
    "#pvt_ent\n",
    "\n",
    "#pvt_com = df_com_prod.pivot_table(values='booking_net', index=['services_indicator'], aggfunc=sum, margins=True)\n",
    "#pvt_com\n",
    "\n",
    "#pvt_ent = df_ent_prod.pivot_table(values='booking_net', index=['services_indicator'], aggfunc=sum, margins=True)\n",
    "#pvt_ent\n",
    "\n",
    "#pvt_anil_ent = df_anil_ent.pivot_table(values='booking_net', index=['services_indicator'], aggfunc=sum, margins=True)\n",
    "#pvt_anil_ent\n",
    "#pvt_anil_com = df_anil_com.pivot_table(values='booking_net', index=['services_indicator'], aggfunc=sum, margins=True)\n",
    "#pvt_anil_com\n",
    "\n",
    "#df_com_prod[pd.isnull(df_com_prod.new_sales_level_6)][['sales_level_6', 'gaining_new_sales_level_6', 'new_sales_level_6']]\n",
    "\n",
    "df_com_prod = create_new_com_cols(df_com_prod)\n",
    "df_ent_prod = create_new_com_cols2(df_ent_prod)\n",
    "#df_ent_prod.columns\n",
    "#df_anil[['sales_level_2', 'gaining_sales_level_2','sales_level_3', 'gaining_sales_level_3','sales_level_4', 'gaining_sales_level_4','sales_level_5', 'gaining_sales_level_5','sales_level_6', 'gaining_sales_level_6']]\n",
    "#df_anil_ent = df_anil[-(df_anil.sales_level_3 == 'INDIA_COMM_1')]\n",
    "#df_anil_com = df_anil[(df_anil.sales_level_3 == 'INDIA_COMM_1')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COM - Before: 163449\n",
      "ENT - Before: 144648\n"
     ]
    }
   ],
   "source": [
    "com_rows_before = len(df_com_prod.index)\n",
    "ent_rows_before = len(df_ent_prod.index)\n",
    "print \"COM - Before: %d\" % (com_rows_before)\n",
    "print \"ENT - Before: %d\" % (ent_rows_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48580"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_com_prod[df_com_prod.fiscal_quarter_id == '2016Q1'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2016Q1...\n",
      "Merging 2016Q1...\n",
      "Before: 48580 | After: 46851\n",
      "Merged 2016Q1!\n",
      "Extracting 2016Q2...\n",
      "Merging 2016Q2...\n",
      "Before: 41502 | After: 40166\n",
      "Merged 2016Q2!\n",
      "Extracting 2016Q3...\n",
      "Merging 2016Q3...\n",
      "Before: 47833 | After: 46322\n",
      "Merged 2016Q3!\n",
      "Extracting 2016Q4...\n",
      "Merging 2016Q4...\n",
      "Before: 25534 | After: 24745\n",
      "Merged 2016Q4!\n",
      "Concatenating ...\n",
      "COM - Before: 163449 | After: 158084\n"
     ]
    }
   ],
   "source": [
    "def mergeChunks(df_left_main, df_right_main):\n",
    "    list_dfs = []\n",
    "    for quarter in ['2016Q1', '2016Q2', '2016Q3', '2016Q4']:\n",
    "        print 'Extracting %s...' % quarter\n",
    "        df_left = df_left_main[df_left_main.fiscal_quarter_id == quarter]\n",
    "        df_right_chunk = df_right_main[df_right_main.fiscal_quarter_id == quarter]\n",
    "        df_right = df_right_chunk[['customer_name', 'sales_order_number_detail', 'bookings_adjustments_description','gaining_sales_level_3', 'gaining_sales_level_4']]\n",
    "        df_right = df_right.drop_duplicates()\n",
    "        print 'Merging %s...' % quarter\n",
    "        rows_before = len(df_left.index)\n",
    "        df_left = df_left.merge(df_right, on=['customer_name', 'sales_order_number_detail', 'bookings_adjustments_description'], how='left')\n",
    "        df_left.drop_duplicates(['customer_name', 'sales_order_number_detail', 'bookings_adjustments_description', 'booking_net'], keep='first', inplace=True)\n",
    "        rows_after = len(df_left.index)\n",
    "        print \"Before: %d | After: %d\" % (rows_before, rows_after)\n",
    "        list_dfs.append(df_left)\n",
    "        print 'Merged %s!' % quarter\n",
    "    print 'Concatenating ...'\n",
    "    df_final = pd.concat(list_dfs)\n",
    "    return df_final\n",
    "        \n",
    "\n",
    "def validateSalesLevel3(row):\n",
    "    df_match = df_anil_com_fields[(df_anil_com_fields.customer_name == row['customer_name']) & (df_anil_com_fields.bookings_adjustments_description == row['bookings_adjustments_description']) & (df_anil_com_fields.sales_order_number_detail == row['sales_order_number_detail'])]\n",
    "    result = df_match['gaining_sales_level_3'].unique()[0] if len(df_match['gaining_sales_level_3'].unique())>0 else 'NOT FOUND'\n",
    "    return result\n",
    "\n",
    "def validateSalesLevel4(row):\n",
    "    df_match = df_anil_com_fields[(df_anil_com_fields.customer_name == row['customer_name']) & (df_anil_com_fields.bookings_adjustments_description == row['bookings_adjustments_description']) & (df_anil_com_fields.sales_order_number_detail == row['sales_order_number_detail'])]\n",
    "    result = df_match['gaining_sales_level_4'].unique()[0] if len(df_match['gaining_sales_level_4'].unique())>0 else 'NOT FOUND'\n",
    "    return result\n",
    "\n",
    "def validate_ENT_SL3(row):\n",
    "    df_match = df_anil_ent_fields[(df_anil_ent_fields.customer_name == row['customer_name']) & (df_anil_ent_fields.bookings_adjustments_description == row['bookings_adjustments_description']) & (df_anil_ent_fields.sales_order_number_detail == row['sales_order_number_detail'])]\n",
    "    result = df_match['gaining_sales_level_3'].unique()[0] if len(df_match['gaining_sales_level_3'].unique())>0 else 'NOT FOUND'\n",
    "    return result\n",
    "\n",
    "def validate_ENT_SL4(row):\n",
    "    df_match = df_anil_ent_fields[(df_anil_ent_fields.customer_name == row['customer_name']) & (df_anil_ent_fields.bookings_adjustments_description == row['bookings_adjustments_description']) & (df_anil_ent_fields.sales_order_number_detail == row['sales_order_number_detail'])]\n",
    "    result = df_match['gaining_sales_level_4'].unique()[0] if len(df_match['gaining_sales_level_4'].unique())>0 else 'NOT FOUND'\n",
    "    return result\n",
    "\n",
    "#df_com_prod['gaining_sales_level_3_y'] = df_com_prod.apply(lambda row: validateSalesLevel3(row), axis=1)\n",
    "#df_com_prod['gaining_sales_level_4_y'] = df_com_prod.apply(lambda row: validateSalesLevel4(row), axis=1)\n",
    "\n",
    "#df_ent_prod['gaining_sales_level_3_y'] = df_com_prod.apply(lambda row: validate_ENT_SL3(row), axis=1)\n",
    "#df_ent_prod['gaining_sales_level_4_y'] = df_com_prod.apply(lambda row: validate_ENT_SL4(row), axis=1)\n",
    "\n",
    "\n",
    "df_com_prod2 = mergeChunks(df_com_prod, df_anil_com)\n",
    "com_rows_after = len(df_com_prod2.index)\n",
    "#ent_rows_after = len(df_ent_prod2.index)\n",
    "print \"COM - Before: %d | After: %d\" % (com_rows_before, com_rows_after)\n",
    "#print \"ENT - Before: %d | After: %d\" % (ent_rows_before, ent_rows_after)\n",
    "\n",
    "#df_anil_com_fields = df_anil_com[['customer_name', 'sales_order_number_detail', 'bookings_adjustments_description','gaining_sales_level_3', 'gaining_sales_level_4']]\n",
    "#df_anil_ent_fields = df_anil_ent[['customer_name', 'sales_order_number_detail', 'bookings_adjustments_description','gaining_sales_level_3', 'gaining_sales_level_4']]\n",
    "#len(df_anil_com_fields.index)\n",
    "#df_anil_com_fields.gaining_sales_level_4.unique()[0]\n",
    "#df_anil_com = df_anil_com.drop_duplicates()\n",
    "#df_com_prod2 = df_com_prod.merge(df_anil_com_fields, on=['customer_name', 'sales_order_number_detail', 'bookings_adjustments_description'], how='left')\n",
    "#df_ent_prod2 = df_ent_prod.merge(df_anil_ent_fields, on=['customer_name', 'sales_order_number_detail', 'bookings_adjustments_description'], how='left')\n",
    "#com_rows_after = len(df_com_prod.index)\n",
    "#ent_rows_after = len(df_ent_prod.index)\n",
    "#print \"COM - Before: %d | After: %d\" % (com_rows_before, com_rows_after)\n",
    "#print \"ENT - Before: %d | After: %d\" % (ent_rows_before, ent_rows_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2016 = pd.concat([df_com_prod, df_ent_prod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2016.to_csv('../booking_mapped_2016.csv', sep=',', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
